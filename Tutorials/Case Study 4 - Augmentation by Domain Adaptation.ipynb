{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 - Augmentation\n",
    "\n",
    "## The Task\n",
    "Augment a small dataset using the concept of domain adaptation (or transfer learning). For this we will be using a RadialGAN as discussed in [this paper](https://arxiv.org/pdf/1802.06403.pdf).\n",
    "\n",
    "### Imports\n",
    "Lets import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "from synthcity.utils import serialization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the level for the logging\n",
    "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "log.remove()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scenario\n",
    "\n",
    "Brazil is divided geopolitically into five macroregions: north, northeast, central-west, southeast, and south. For this case study, we will be acting as government officials in the Central-West Region of Brazil. Central-West Brazil is the smallest region in the country by population. It is also one of the larger and more rural regions. This means the number of COVID-19 patient records is significantly smaller compared to the larger regions.\n",
    "\n",
    "<img src=\"../data/Brazil_COVID/Brazil_Labelled_Map.png\" alt=\"Brazil Region Map\" width=\"400\"/>\n",
    "\n",
    "COVID-19 hit different regions at different time. Cases peaked later in the Central-West than in the more densely-populated and well-connected regions. Giving us the problem of scarce data in terms of COVID-19 patients in the region, but the potential lifeline of having larger datasets from the other regions, which we can learn from in order to augment our dataset. We cannot simply train our model on the data from all regions, because there is significant co-variate shift between the different regions and so we will achieve a better classifier by training on solely Central-West data, even if it is synthetic. \n",
    "\n",
    "### Load the data\n",
    "Lets set it up as a classification task with a death at time horizon column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Southeast       4929\n",
      "Northeast        714\n",
      "South            624\n",
      "North            510\n",
      "Central-West     105\n",
      "Name: Region, dtype: int64\n",
      "1    6777\n",
      "0     105\n",
      "Name: Region, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Sore_throat</th>\n",
       "      <th>Shortness_of_breath</th>\n",
       "      <th>Respiratory_discomfort</th>\n",
       "      <th>SPO2</th>\n",
       "      <th>...</th>\n",
       "      <th>Cardiovascular</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>Diabetis</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>Immunosuppresion</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Neurologic</th>\n",
       "      <th>Renal</th>\n",
       "      <th>is_dead_at_time_horizon=14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Sex  Ethnicity  Region  Fever  Cough  Sore_throat  \\\n",
       "32     37    1          0       0      1      1            1   \n",
       "33     62    1          0       0      1      1            0   \n",
       "42     56    1          1       0      1      1            0   \n",
       "44     25    0          0       0      0      1            1   \n",
       "45     27    1          0       0      1      1            1   \n",
       "...   ...  ...        ...     ...    ...    ...          ...   \n",
       "6818   58    1          0       0      1      1            0   \n",
       "6819   63    0          1       0      1      1            0   \n",
       "6820   30    1          1       0      1      1            0   \n",
       "6821   38    1          1       0      0      0            0   \n",
       "6836   99    1          1       0      1      1            0   \n",
       "\n",
       "      Shortness_of_breath  Respiratory_discomfort  SPO2  ...  Cardiovascular  \\\n",
       "32                      1                       1     0  ...               0   \n",
       "33                      1                       1     1  ...               0   \n",
       "42                      1                       1     1  ...               0   \n",
       "44                      0                       0     0  ...               0   \n",
       "45                      1                       1     1  ...               0   \n",
       "...                   ...                     ...   ...  ...             ...   \n",
       "6818                    0                       0     0  ...               0   \n",
       "6819                    1                       1     0  ...               0   \n",
       "6820                    1                       1     1  ...               0   \n",
       "6821                    0                       0     0  ...               0   \n",
       "6836                    1                       1     1  ...               1   \n",
       "\n",
       "      Asthma  Diabetis  Pulmonary  Immunosuppresion  Obesity  Liver  \\\n",
       "32         0         0          0                 0        0      0   \n",
       "33         0         0          0                 0        0      0   \n",
       "42         0         0          0                 0        0      0   \n",
       "44         0         0          0                 0        0      0   \n",
       "45         0         0          0                 0        0      0   \n",
       "...      ...       ...        ...               ...      ...    ...   \n",
       "6818       0         0          0                 0        0      0   \n",
       "6819       0         0          0                 0        0      0   \n",
       "6820       0         0          0                 0        0      0   \n",
       "6821       0         0          0                 0        0      0   \n",
       "6836       0         0          1                 0        0      0   \n",
       "\n",
       "      Neurologic  Renal  is_dead_at_time_horizon=14  \n",
       "32             0      0                           0  \n",
       "33             0      0                           0  \n",
       "42             0      0                           0  \n",
       "44             0      0                           0  \n",
       "45             0      0                           0  \n",
       "...          ...    ...                         ...  \n",
       "6818           0      0                           0  \n",
       "6819           0      0                           0  \n",
       "6820           0      0                           0  \n",
       "6821           0      0                           0  \n",
       "6836           1      1                           0  \n",
       "\n",
       "[105 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_horizon = 14\n",
    "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
    "\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
    "\n",
    "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem\n",
    "\n",
    "# Define the mappings from region index to region\n",
    "region_mapper = {\n",
    "    0: \"Central-West\",\n",
    "    1: \"North\",\n",
    "    2: \"Northeast\",\n",
    "    3: \"South\",\n",
    "    4: \"Southeast\",\n",
    "}\n",
    "our_region_index = 0\n",
    "print(X[\"Region\"].value_counts().rename(region_mapper))\n",
    "\n",
    "# Flatten region to simulate the scenario where we don't know where the data has come from, we just have our data and other data\n",
    "other_region_index = 1 if our_region_index != 1 else 0\n",
    "X.loc[X[\"Region\"] != our_region_index, \"Region\"] = other_region_index\n",
    "\n",
    "X_our_region_only = X.loc[X[\"Region\"] == our_region_index].copy()\n",
    "X_other_regions = X.loc[X[\"Region\"] != our_region_index].copy()\n",
    "\n",
    "display(X_our_region_only)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem\n",
    "\n",
    "Lets train a model on just our data from the Central-West region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: train set: 0.9102564102564102 | test set: 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are significantly overfitting due to the very small dataset.\n",
    "\n",
    "### Now lets test our assertion that we can't just use all the training data and apply it to our region\n",
    "As you can see our accuracy does improve, but we can do better! And there may well be cases where there is a greater co-variate shift that impacts this accuracy to a much greater extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: train set: 0.8015888393722147 | test set: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "y = X[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, _, y_train, _ = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution\n",
    "\n",
    "Lets augment this dataset with the use of a RadialGAN.\n",
    "\n",
    "First, lets load the super-set of data from all regions into the GerericDataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6777\n",
      "0     105\n",
      "Name: Region, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "loader = GenericDataLoader(\n",
    "    X, # X is the dataframe which is a superset of all region data\n",
    "    target_column=\"is_dead_at_time_horizon=14\", # The column containing the labels to predict\n",
    "    sensitive_features=[\"Age\", \"Sex\", \"Ethnicity\", \"Region\"], # The sensitive features in the dataset (Not needed here?)\n",
    "    domain_column=\"Region\", # This labels the domain that each record is from. Where it is `0` it is from our small dataset.\n",
    "    random_state=42,\n",
    ")\n",
    "print(loader.info())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a RadialGan to augment the data. We need to load the plugin and then fit it to the dataloader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radialgan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:08<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "outdir = Path(\"saved_models\")\n",
    "prefix = \"Augmentation\"\n",
    "model=\"radialgan\"\n",
    "n_iter = 15\n",
    "\n",
    "print(model)\n",
    "\n",
    "save_file = outdir / f\"{prefix}.{model}_numericalised_{region_mapper[our_region_index]}_n_iter={n_iter}.bkp\"\n",
    "if Path(save_file).exists():\n",
    "    syn_model = serialization.load_from_file(save_file)\n",
    "else:\n",
    "    syn_model = Plugins().get(model, n_iter=n_iter)\n",
    "    syn_model.fit(loader)\n",
    "    syn_model.generate(count=6882).dataframe()\n",
    "    serialization.save_to_file(save_file, syn_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "Lets train the model on an augmented dataset and see what our performance is now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: n_gen_records: 1000 train set: 0.9106280193236715| test set: 0.924187725631769\n"
     ]
    }
   ],
   "source": [
    "n_gen_records = 1000\n",
    "\n",
    "synth_data = syn_model.generate(n_gen_records, domains=[our_region_index])\n",
    "\n",
    "# Now we can augment our original dataset with our new synthetic data\n",
    "augmented_data = pd.concat([\n",
    "    synth_data.dataframe(),\n",
    "    X_our_region_only,\n",
    "])\n",
    "\n",
    "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
    "augmented_X_in = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_X_in, augmented_y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: n_gen_records: {n_gen_records} train set: {calculated_accuracy_score_train}| test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model over-fitting on the training data is significantly reduced and the accuracy that is much higher than for the small dataset comprised solely of data from the Central-West region. We also see a significant improvement over training the model on the superset of the real data.\n",
    "\n",
    "\n",
    "``` - Artificially drop some features to prohibit merging of the dataset?```\n",
    "\n",
    "### Can you generate some more augmented datasets to answer the foloowing questions?\n",
    " - How much synthetic data should you create for best results?\n",
    " - How much does changing the RadialGan plugin parameter `n_iter` change the quality of the generated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracies = []\n",
    "generated_records = [\n",
    "    10,20,30,40,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000,8000,10000,15000,20000\n",
    "]\n",
    "repeats = 10\n",
    "for n_gen_records in generated_records:\n",
    "    rep_vals = []\n",
    "    for i in range(repeats):\n",
    "        synth_data = syn_model.generate(n_gen_records, domains=[our_region_index])\n",
    "\n",
    "        # Now we can augment our original dataset with our new synthetic data\n",
    "        augmented_data = pd.concat([\n",
    "            synth_data.dataframe(),\n",
    "            X_our_region_only,\n",
    "        ])\n",
    "\n",
    "        augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
    "        augmented_X_in = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(augmented_X_in, augmented_y, random_state=4)\n",
    "        X_train.reset_index(drop=True, inplace=True)\n",
    "        X_test.reset_index(drop=True, inplace=True)\n",
    "        y_train.reset_index(drop=True, inplace=True)\n",
    "        y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Train model on whole dataset\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=2000,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=5,\n",
    "            subsample=0.8, \n",
    "            colsample_bytree=1, \n",
    "            gamma=1, \n",
    "            objective=\"binary:logistic\",\n",
    "            random_state=42,\n",
    "        )\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "        calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "        # print(f\"Evaluating accuracy: n_gen_records: {n_gen_records} train set: {calculated_accuracy_score_train}| test set: {calculated_accuracy_score_test}\")\n",
    "        rep_vals.append(calculated_accuracy_score_test)\n",
    "    accuracies.append(np.mean(rep_vals))\n",
    "\n",
    "d = {\"generated_records\": generated_records, \"accuracies\": accuracies}\n",
    "accuracy_data= pd.DataFrame(d)\n",
    "plot = sns.lineplot(\n",
    "    y=\"accuracies\",\n",
    "    x=\"generated_records\",\n",
    "    data=accuracy_data\n",
    ").set(title=f\"Augmenting {region_mapper[our_region_index]}, n_iter={500}, without {'original data'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
