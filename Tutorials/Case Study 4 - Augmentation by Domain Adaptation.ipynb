{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 - Augmentation\n",
    "\n",
    "TODO: Whole dataset model performance too high...\n",
    "\n",
    "## The Task\n",
    "Augment a small dataset using the concept of domain adaptation (or transfer learning). For this we will be using a RadialGAN as discussed in [this paper](https://arxiv.org/pdf/1802.06403.pdf).\n",
    "\n",
    "### Imports\n",
    "Lets import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniconda3/envs/synth-lab/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# stdlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "from synthcity.utils import serialization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the level for the logging\n",
    "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "log.remove()\n",
    "\n",
    "# Set up paths to resources\n",
    "AUG_RES_PATH = Path(\"../resources/augmentation/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scenario\n",
    "\n",
    "Brazil is divided geopolitically into five macroregions: north, northeast, central-west, southeast, and south. For this case study, we will be acting as government officials in the Central-West Region of Brazil. Central-West Brazil is the smallest region in the country by population. It is also one of the larger and more rural regions. This means the number of COVID-19 patient records is significantly smaller compared to the larger regions.\n",
    "\n",
    "<img src=\"../data/Brazil_COVID/Brazil_Labelled_Map.png\" alt=\"Brazil Region Map\" width=\"400\"/>\n",
    "\n",
    "COVID-19 hit different regions at different time. Cases peaked later in the Central-West than in the more densely-populated and well-connected regions. Giving us the problem of scarce data in terms of COVID-19 patients in the region, but the potential lifeline of having larger datasets from the other regions, which we can learn from in order to augment our dataset. We cannot simply train our model on the data from all regions, because there is significant co-variate shift between the different regions and so we will achieve a better classifier by training on solely Central-West data, even if it is synthetic. \n",
    "\n",
    "### Load the data\n",
    "Lets set it up as a classification task with a death at time horizon column, as we did in a previous case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_horizon = 14\n",
    "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
    "\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
    "\n",
    "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a region mapper, which maps the region encoding to the real values. These can be found in `synthetic-data-lab/data/Brazil_COVID/Brazil_COVID_data.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mappings from region index to region\n",
    "region_mapper = {\n",
    "    0: \"Central-West\",\n",
    "    1: \"North\",\n",
    "    2: \"Northeast\",\n",
    "    3: \"South\",\n",
    "    4: \"Southeast\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are acting as officials from Central-West Brazil, split the data into data from our region and data from other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another region    6777\n",
      "Our region         105\n",
      "Name: Region, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Sore_throat</th>\n",
       "      <th>Shortness_of_breath</th>\n",
       "      <th>Respiratory_discomfort</th>\n",
       "      <th>SPO2</th>\n",
       "      <th>...</th>\n",
       "      <th>Cardiovascular</th>\n",
       "      <th>Asthma</th>\n",
       "      <th>Diabetis</th>\n",
       "      <th>Pulmonary</th>\n",
       "      <th>Immunosuppresion</th>\n",
       "      <th>Obesity</th>\n",
       "      <th>Liver</th>\n",
       "      <th>Neurologic</th>\n",
       "      <th>Renal</th>\n",
       "      <th>is_dead_at_time_horizon=14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6821</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Sex  Ethnicity  Region  Fever  Cough  Sore_throat  \\\n",
       "32     37    1          0       0      1      1            1   \n",
       "33     62    1          0       0      1      1            0   \n",
       "42     56    1          1       0      1      1            0   \n",
       "44     25    0          0       0      0      1            1   \n",
       "45     27    1          0       0      1      1            1   \n",
       "...   ...  ...        ...     ...    ...    ...          ...   \n",
       "6818   58    1          0       0      1      1            0   \n",
       "6819   63    0          1       0      1      1            0   \n",
       "6820   30    1          1       0      1      1            0   \n",
       "6821   38    1          1       0      0      0            0   \n",
       "6836   99    1          1       0      1      1            0   \n",
       "\n",
       "      Shortness_of_breath  Respiratory_discomfort  SPO2  ...  Cardiovascular  \\\n",
       "32                      1                       1     0  ...               0   \n",
       "33                      1                       1     1  ...               0   \n",
       "42                      1                       1     1  ...               0   \n",
       "44                      0                       0     0  ...               0   \n",
       "45                      1                       1     1  ...               0   \n",
       "...                   ...                     ...   ...  ...             ...   \n",
       "6818                    0                       0     0  ...               0   \n",
       "6819                    1                       1     0  ...               0   \n",
       "6820                    1                       1     1  ...               0   \n",
       "6821                    0                       0     0  ...               0   \n",
       "6836                    1                       1     1  ...               1   \n",
       "\n",
       "      Asthma  Diabetis  Pulmonary  Immunosuppresion  Obesity  Liver  \\\n",
       "32         0         0          0                 0        0      0   \n",
       "33         0         0          0                 0        0      0   \n",
       "42         0         0          0                 0        0      0   \n",
       "44         0         0          0                 0        0      0   \n",
       "45         0         0          0                 0        0      0   \n",
       "...      ...       ...        ...               ...      ...    ...   \n",
       "6818       0         0          0                 0        0      0   \n",
       "6819       0         0          0                 0        0      0   \n",
       "6820       0         0          0                 0        0      0   \n",
       "6821       0         0          0                 0        0      0   \n",
       "6836       0         0          1                 0        0      0   \n",
       "\n",
       "      Neurologic  Renal  is_dead_at_time_horizon=14  \n",
       "32             0      0                           0  \n",
       "33             0      0                           0  \n",
       "42             0      0                           0  \n",
       "44             0      0                           0  \n",
       "45             0      0                           0  \n",
       "...          ...    ...                         ...  \n",
       "6818           0      0                           0  \n",
       "6819           0      0                           0  \n",
       "6820           0      0                           0  \n",
       "6821           0      0                           0  \n",
       "6836           1      1                           0  \n",
       "\n",
       "[105 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "our_region_index = 0\n",
    "\n",
    "# Flatten region to simulate the scenario where we don't know where the data has come from, we just have our data and other data\n",
    "other_region_index = 1 if our_region_index != 1 else 0\n",
    "X.loc[X[\"Region\"] != our_region_index, \"Region\"] = other_region_index\n",
    "print(X[\"Region\"].value_counts().rename({our_region_index: \"Our region\", 1:\"Another region\"}))\n",
    "\n",
    "X_our_region_only = X.loc[X[\"Region\"] == our_region_index].copy()\n",
    "X_other_regions = X.loc[X[\"Region\"] != our_region_index].copy()\n",
    "\n",
    "display(X_our_region_only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem\n",
    "\n",
    "Lets see how a model trained just on our data from the Central-West region performs.\n",
    "\n",
    "Set up the data splits using train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained xgboost classifier, which has been trained on Central-West data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_{region_mapper[our_region_index]}_data.json\"\n",
    "xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "# xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the performance of the model trained on only Central-West data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: train set: 0.9102564102564102 | test set: 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are significantly over-fitting due to the very small dataset.\n",
    "\n",
    "### Now lets test our assertion that we can't just use all the training data and apply it to our region\n",
    "\n",
    "Set up the training and testing data sets for the model, making sure the training sets come from the all region dataset, but the test sets come from our region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, _, y_train, _ = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model trained on data from all regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_{region_mapper[our_region_index]}_data.json\"\n",
    "xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "# xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: train set: 0.8006200348769619 | test set: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our accuracy does improve, but we can do better! And there may well be cases where there is a greater co-variate shift that impacts this accuracy to a much greater extent. It is also worth bearing in mind that there are contexts where the above approach is not even an option, such as in the case of only partially overlapping (or missing) features.\n",
    "\n",
    "# The Solution\n",
    "\n",
    "Lets augment this dataset with the use of a RadialGAN.\n",
    "\n",
    "First, lets load the super-set of data from all regions into the GerericDataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericDataLoader(\n",
    "    X, # X is the dataframe which is a superset of all region data\n",
    "    target_column=\"is_dead_at_time_horizon=14\", # The column containing the labels to predict\n",
    "    sensitive_features=[\"Age\", \"Sex\", \"Ethnicity\", \"Region\"], # The sensitive features in the dataset (Not needed here?)\n",
    "    domain_column=\"Region\", # This labels the domain that each record is from. Where it is `0` it is from our small dataset.\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a RadialGan to augment the data. We need to load the plugin and then fit it to the dataloader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radialgan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:07<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "outdir = Path(\"saved_models\")\n",
    "prefix = \"Augmentation\"\n",
    "model=\"radialgan\"\n",
    "n_iter = 15\n",
    "random_state = 42\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Define saved model name\n",
    "save_file = outdir / f\"{prefix}.{model}_numericalised_{region_mapper[our_region_index]}_n_iter={n_iter}_rnd={random_state}.bkp\"\n",
    "# Load if available\n",
    "if Path(save_file).exists():\n",
    "    syn_model = serialization.load_from_file(save_file)\n",
    "# create and fit if not available\n",
    "else:\n",
    "    syn_model = Plugins().get(model, n_iter=n_iter, random_state=random_state)\n",
    "    syn_model.fit(loader)\n",
    "    serialization.save_to_file(save_file, syn_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "Lets train the model on an augmented dataset and see what our performance is now.\n",
    "\n",
    "Lets use our synthetic model to generate some data and use it to augment our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen_records = 1000\n",
    "\n",
    "synth_data = syn_model.generate(n_gen_records, domains=[our_region_index], random_state=random_state)\n",
    "\n",
    "# Now we can augment our original dataset with our new synthetic data\n",
    "augmented_data = pd.concat([\n",
    "    synth_data.dataframe(),\n",
    "    X_our_region_only,\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to test a model trained on the augmented dataset, so we need to set up our data splits again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
    "augmented_X_in = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_X_in, augmented_y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_augmented_data.json\"\n",
    "xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "# xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the performance of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating accuracy: n_gen_records: 1000 train set: 0.9251207729468599 | test set: 0.9169675090252708\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: n_gen_records: {n_gen_records} train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model over-fitting on the training data is significantly reduced and the accuracy that is much higher than for the small dataset comprised solely of data from the Central-West region. We also see a significant improvement over training the model on the superset of the real data.\n",
    "\n",
    "### Can you generate some more augmented datasets to answer the foloowing questions?\n",
    " - How much synthetic data should you create for best results?\n",
    " - How much does changing the RadialGan plugin parameter `n_iter` change the quality of the generated data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
