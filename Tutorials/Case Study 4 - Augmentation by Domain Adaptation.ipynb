{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 - Augmentation\n",
    "\n",
    "## The Task\n",
    "Augment a small dataset using the concept of domain adaptation (or transfer learning). For this we will be using a RadialGAN as discussed in [this paper](https://arxiv.org/pdf/1802.06403.pdf).\n",
    "\n",
    "### Imports\n",
    "Lets import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "from synthcity.utils import serialization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the level for the logging\n",
    "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "log.remove()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scenario\n",
    "\n",
    "Brazil is divided geopolitically into five macroregions: north, northeast, central-west, southeast, and south. For this case study, we will be acting as government officials in the Central-West Region of Brazil. Central-West Brazil is the smallest region in the country by population. It is also one of the larger and more rural regions. This means the number of COVID-19 patient records is significantly smaller compared to the larger regions.\n",
    "\n",
    "<img src=\"../data/Brazil_COVID/Brazil_Labelled_Map.png\" alt=\"Brazil Region Map\" width=\"400\"/>\n",
    "\n",
    "COVID-19 hit different regions at different time. Cases peaked later in the Central-West than in the more densely-populated and well-connected regions. Giving us the problem of scarce data in terms of COVID-19 patients in the region, but the potential lifeline of having larger datasets from the other regions, which we can learn from in order to augment our dataset. We cannot simply train our model on the data from all regions, because there is significant co-variate shift between the different regions and so we will achieve a better classifier by training on solely Central-West data, even if it is synthetic. \n",
    "\n",
    "### Load the data\n",
    "Lets set it up as a classification task with a death at time horizon column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_horizon = 14\n",
    "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
    "\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
    "\n",
    "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem\n",
    "\n",
    "# Define the mappings from region index to region\n",
    "region_mapper = {\n",
    "    0: \"Central-West\",\n",
    "    1: \"North\",\n",
    "    2: \"Northeast\",\n",
    "    3: \"South\",\n",
    "    4: \"Southeast\",\n",
    "}\n",
    "our_region_index = 0\n",
    "print(X[\"Region\"].value_counts().rename(region_mapper))\n",
    "\n",
    "# Flatten region to simulate the scenario where we don't know where the data has come from, we just have our data and other data\n",
    "other_region_index = 1 if our_region_index != 1 else 0\n",
    "X.loc[X[\"Region\"] != our_region_index, \"Region\"] = other_region_index\n",
    "\n",
    "X_our_region_only = X.loc[X[\"Region\"] == our_region_index].copy()\n",
    "X_other_regions = X.loc[X[\"Region\"] != our_region_index].copy()\n",
    "\n",
    "display(X_our_region_only)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem\n",
    "\n",
    "Lets train a model on just our data from the Central-West region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are significantly over-fitting due to the very small dataset.\n",
    "\n",
    "### Now lets test our assertion that we can't just use all the training data and apply it to our region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, _, y_train, _ = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our accuracy does improve, but we can do better! And there may well be cases where there is a greater co-variate shift that impacts this accuracy to a much greater extent. It is also worth bearing in mind that there are contexts where the above approach is not even an option, such as in the case of only partially overlapping (or missing) features.\n",
    "\n",
    "# The Solution\n",
    "\n",
    "Lets augment this dataset with the use of a RadialGAN.\n",
    "\n",
    "First, lets load the super-set of data from all regions into the GerericDataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericDataLoader(\n",
    "    X, # X is the dataframe which is a superset of all region data\n",
    "    target_column=\"is_dead_at_time_horizon=14\", # The column containing the labels to predict\n",
    "    sensitive_features=[\"Age\", \"Sex\", \"Ethnicity\", \"Region\"], # The sensitive features in the dataset (Not needed here?)\n",
    "    domain_column=\"Region\", # This labels the domain that each record is from. Where it is `0` it is from our small dataset.\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a RadialGan to augment the data. We need to load the plugin and then fit it to the dataloader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(\"saved_models\")\n",
    "prefix = \"Augmentation\"\n",
    "model=\"radialgan\"\n",
    "n_iter = 15\n",
    "\n",
    "print(model)\n",
    "\n",
    "save_file = outdir / f\"{prefix}.{model}_numericalised_{region_mapper[our_region_index]}_n_iter={n_iter}.bkp\"\n",
    "if Path(save_file).exists():\n",
    "    syn_model = serialization.load_from_file(save_file)\n",
    "else:\n",
    "    syn_model = Plugins().get(model, n_iter=n_iter)\n",
    "    syn_model.fit(loader)\n",
    "    syn_model.generate(count=6882).dataframe()\n",
    "    serialization.save_to_file(save_file, syn_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "Lets train the model on an augmented dataset and see what our performance is now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen_records = 1000\n",
    "\n",
    "synth_data = syn_model.generate(n_gen_records, domains=[our_region_index])\n",
    "\n",
    "# Now we can augment our original dataset with our new synthetic data\n",
    "augmented_data = pd.concat([\n",
    "    synth_data.dataframe(),\n",
    "    X_our_region_only,\n",
    "])\n",
    "\n",
    "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
    "augmented_X_in = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_X_in, augmented_y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Train model on whole dataset\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: n_gen_records: {n_gen_records} train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model over-fitting on the training data is significantly reduced and the accuracy that is much higher than for the small dataset comprised solely of data from the Central-West region. We also see a significant improvement over training the model on the superset of the real data.\n",
    "\n",
    "### Can you generate some more augmented datasets to answer the foloowing questions?\n",
    " - How much synthetic data should you create for best results?\n",
    " - How much does changing the RadialGan plugin parameter `n_iter` change the quality of the generated data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
