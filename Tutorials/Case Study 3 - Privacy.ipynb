{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97e2d93c",
   "metadata": {},
   "source": [
    "# Case Study 3 - Privacy\n",
    "\n",
    "## The Task\n",
    "Make a private version of the Brazil COVID-19 dataset, that could safely be used by anyone to create a COVID-19 survival analysis model, without the risk of (re-)identification of individuals.\n",
    "\n",
    "### Imports\n",
    "Lets get the imports out of the way. We import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here, using Synthcity's bespoke logger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e0157",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# synthcity\n",
    "import synthcity.logger as log\n",
    "from synthcity.utils import serialization\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import (GenericDataLoader, SurvivalAnalysisDataLoader)\n",
    "from synthcity.metrics import Metrics\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the level for the logging\n",
    "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "log.remove()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afc77560",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Load the data from file into a SurvivalAnalysisDataLoader object. For this we need to pass the names of our `target_column` and our `time_to_event_column` to the data loader. Then we can see the data by calling loader.dataframe() and get the infomation about the data loader object with loader.info()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51076cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
    "loader = SurvivalAnalysisDataLoader(\n",
    "    X,\n",
    "    target_column=\"is_dead\",\n",
    "    time_to_event_column=\"Days_hospital_to_outcome\",\n",
    "    sensitive_features=[\"Age\", \"Sex\", \"Ethnicity\", \"Region\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "display(loader.dataframe())\n",
    "# display(loader.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1ea129",
   "metadata": {},
   "source": [
    "## Synthetic generators\n",
    "\n",
    "We can list the available generic synthetic generators by calling list() on the Plugins object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Plugins(categories=[\"generic\", \"survival_analysis\"]).list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "229d9fbe",
   "metadata": {},
   "source": [
    "### Create synthetic datasets\n",
    "\n",
    "From the above list we are going to select the synthetic generation models for privacy: \"privbayes\", \"dpgan\", \"adsgan\", and \"pategan\". Then we will create and fit the synthetic model before using it to generate a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(\"saved_models\")\n",
    "prefix = \"privacy\"\n",
    "n_iter = 100\n",
    "models=[\n",
    "    \"dpgan\",\n",
    "    \"adsgan\",\n",
    "    \"pategan\",\n",
    "]\n",
    "for model in models:\n",
    "    save_file = outdir / f\"{prefix}.{model}_numericalised_n_iter={n_iter}_4.bkp\"\n",
    "    if not Path(save_file).exists():\n",
    "        print(model)\n",
    "        syn_model = Plugins().get(model)\n",
    "        syn_model.fit(loader)\n",
    "        syn_model.generate(count=6882).dataframe()\n",
    "        serialization.save_to_file(save_file, syn_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70a1cb76",
   "metadata": {},
   "source": [
    "### Evaluate the generated synthetic dataset in terms of privacy\n",
    "\n",
    "We can select some metrics to choose. The full list of available metrics can be seen by calling Metrics().list(). We are going to use the metrics associated with detection of the synthetic data and data privacy. Then we will print them to a dataframe to look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b938f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "for model in models:\n",
    "    print(model)\n",
    "    save_file = outdir / f\"{prefix}.{model}_numericalised_n_iter={n_iter}_4.bkp\"\n",
    "    if Path(save_file).exists():\n",
    "        syn_model = serialization.load_from_file(save_file)\n",
    "        selected_metrics = {\n",
    "            \"detection\": [\"detection_xgb\", \"detection_mlp\", \"detection_gmm\"],\n",
    "            \"privacy\": [\"delta-presence\", \"k-anonymization\", \"k-map\", \"distinct l-diversity\", \"identifiability_score\"],\n",
    "            'performance': ['linear_model', 'mlp', 'xgb', 'feat_rank_distance'],\n",
    "        }\n",
    "        my_metrics = Metrics()\n",
    "        selected_metrics_in_my_metrics = {k: my_metrics.list()[k] for k in my_metrics.list().keys() & selected_metrics.keys()}\n",
    "        X_syn = syn_model.generate(count=6882)\n",
    "        evaluation = my_metrics.evaluate(\n",
    "            loader,\n",
    "            X_syn,\n",
    "            task_type=\"survival_analysis\",\n",
    "            metrics=selected_metrics_in_my_metrics,\n",
    "            workspace=\"workspace\",\n",
    "        )\n",
    "        display(evaluation)\n",
    "        eval_results[model] = evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79e43b62",
   "metadata": {},
   "source": [
    "The above table contains all the infomation we need to evaluate the methods, but lets convert it to a format where it is easier to compare the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024dd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "for plugin in eval_results:\n",
    "    data = eval_results[plugin][\"mean\"]\n",
    "    directions = eval_results[plugin][\"direction\"].to_dict()\n",
    "    means.append(data)\n",
    "\n",
    "out = pd.concat(means, axis=1)\n",
    "out.set_axis(eval_results.keys(), axis=1, inplace=True)\n",
    "\n",
    "bad_highlight = \"background-color: lightcoral;\"\n",
    "ok_highlight = \"background-color: green;\"\n",
    "default = \"\"\n",
    "\n",
    "\n",
    "def highlights(row):\n",
    "    metric = row.name\n",
    "    if directions[metric] == \"minimize\":\n",
    "        best_val = np.min(row.values)\n",
    "        worst_val = np.max(row)\n",
    "    else:\n",
    "        best_val = np.max(row.values)\n",
    "        worst_val = np.min(row)\n",
    "\n",
    "    styles = []\n",
    "    for val in row.values:\n",
    "        if val == best_val:\n",
    "            styles.append(ok_highlight)\n",
    "        elif val == worst_val:\n",
    "            styles.append(bad_highlight)\n",
    "        else:\n",
    "            styles.append(default)\n",
    "\n",
    "    return styles\n",
    "\n",
    "\n",
    "out.style.apply(highlights, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f645dd61",
   "metadata": {},
   "source": [
    "### Results of evaluation\n",
    "\n",
    "We are using two types of metric here to dicsuss privacy: detection and privacy. Detection metrics measure the ability to identify the real data compared to the synthetic data. This has impacts on privacy as if an attacker can identify the real patients in a dataset they can then go about using the subset of real records to try and re-identify the real individuals, i.e. ability to identify the real records reduces the chance of a patient being lost in a crowd of similar synthetic records. The privacy metrics measure how easy it would be to re-identify a patient given the quasi-identifying fields in the dataset.\n",
    "Generally, ADSGAN performs best in synthetic data detection tasks, then PATEGAN, and DPGAN tends to perform very poorly.\n",
    "\n",
    "k-anonymization - risk of re-identification is approximately 1/k according to [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2528029/). Therefore the risk of re-identification is < 2% for DPGAN or ADSGAN and for PATEGAN it is < 3%. In any case there is huge improvement from ground truth k=3. For PATEGAN there is >11-fold increase in k.\n",
    "\n",
    "k-map - is a metric where every combination of values for the quasi-identifiers appears at least k times in the synthetic dataset. ADSGAN performs worse than PATEGAN, but DPGAN comes out on top.\n",
    "\n",
    "l-diversity - Is a similar metric to k-anonymization, but ir is also concerned with the diversity of the generalized block. We see the same pattern as for k-anonymization.\n",
    "\n",
    "identifiability_score - Risk of re-identification as defined in [this paper](https://ieeexplore.ieee.org/document/9034117). This is the best for DPGAN. ADSGAN and PATEGAN perform worse.\n",
    "\n",
    "### Conclusion\n",
    "Generally, it seems DPGAN performs best in the privacy metrics, but the synthetic data is completely distinguishable from the real data by multiple detection algorithms, significantly reducing its utility. ADSGAN performs best in the detection metrics such that detection is not much better than random chance, with PATEGAN second best. ADSGAN and PATEGAN perform better in the detection metrics, but worse in privacy. These need balancing up to find the best solution for your use case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e451888",
   "metadata": {},
   "source": [
    "## Synthetic Data Quality\n",
    "\n",
    "To get a good sense of the quality of the synthetic datasets and validate our previous conclusion. Lets plot the correlation/strength-of-association of features in data-set with both categorical and continuous features using:\n",
    "- Pearson's R for continuous-continuous cases\n",
    "- Correlation Ratio for categorical-continuous cases\n",
    "- Cramer's V or Theil's U for categorical-categorical cases\n",
    "\n",
    "In each of the following plots we are looking for the synthetic data to be as similar to the real data as possible. That is minimal values for Jensen-Shannon distance and pairwise correlation distance, and T-SNEs with similar looking distribution in the representation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for model in models:\n",
    "    print(model)\n",
    "    save_file = outdir / f\"{prefix}.{model}_numericalised_4.bkp\"\n",
    "    if Path(save_file).exists():\n",
    "        syn_model = serialization.load_from_file(save_file)\n",
    "        syn_model.plot(plt, loader, plots=[\"associations\",\"marginal\", \"tsne\"])\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c90f22f4",
   "metadata": {},
   "source": [
    "## Training models on both sets of data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3cbf699",
   "metadata": {},
   "source": [
    "Please now train your own model on both the original dataset and each of the private datasets we have generated to see if you reach the same conclusion. Which privacy method provides the best performance and what are the trade-offs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
