{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 4 - Augmentation\n",
    "These notebooks are also available on Google Colab. This enables you to run the notebooks without having to set up an environment locally and gives you access to GPUs to run the notebooks on.\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JOstMJmhI2wcufyBqZ1iV3YqOdThJ-_U?usp=sharing#scrollTo=4fu5aheXqwv6)\n",
    "\n",
    "## 1. Introduction\n",
    "One of the most common problems for machine Learning practitioners is only having a small dataset for the specific problem they are working on. This traditionally can often lead to dead ends or hold-ups in projects while more data is collected. However, if you have different dataset that shares common features then Synthcity may have the ability to solve the issue for you with \"Augmentation by domain adaption\".\n",
    "\n",
    "### 1.1 The Task\n",
    "Augment a small dataset using the concept of domain adaptation (or transfer learning). For this we will be using a RadialGAN as discussed in [this paper](https://arxiv.org/pdf/1802.06403.pdf).\n",
    "\n",
    "### 2. Imports\n",
    "Lets import the required standard and 3rd party libraries and relevant Synthcity modules. We can also set the level of logging here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# 3rd Party\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, accuracy_score\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# synthcity absolute\n",
    "import synthcity.logger as log\n",
    "from synthcity.plugins import Plugins\n",
    "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
    "from synthcity.utils import serialization\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set the level for the logging\n",
    "# log.add(sink=sys.stderr, level=\"DEBUG\")\n",
    "log.remove()\n",
    "\n",
    "# Set up paths to resources\n",
    "AUG_RES_PATH = Path(\"../resources/augmentation/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Scenario\n",
    "\n",
    "Brazil is divided geopolitically into five macroregions: north, northeast, central-west, southeast, and south. For this case study, we will be acting as government officials in the Central-West Region of Brazil. Central-West Brazil is the smallest region in the country by population. It is also one of the larger and more rural regions. This means the number of COVID-19 patient records is significantly smaller compared to the larger regions.\n",
    "\n",
    "<img src=\"../data/Brazil_COVID/Brazil_Labelled_Map.png\" alt=\"Brazil Region Map\" width=\"400\"/>\n",
    "\n",
    "COVID-19 hit different regions at different time. Cases peaked later in the Central-West than in the more densely-populated and well-connected regions. Giving us the problem of scarce data in terms of COVID-19 patients in the region, but the potential lifeline of having larger datasets from the other regions, which we can learn from in order to augment our dataset. We cannot simply train our model on the data from all regions, because there is significant co-variate shift between the different regions and so we will achieve a better classifier by training on solely Central-West data, even if it is synthetic. \n",
    "\n",
    "### 4. Load the data\n",
    "Lets set it up as a classification task with a death at time horizon column, as we did in a previous case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_horizon = 14\n",
    "X = pd.read_csv(f\"../data/Brazil_COVID/covid_normalised_numericalised.csv\")\n",
    "\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] <= time_horizon) & (X[\"is_dead\"] == 1), f\"is_dead_at_time_horizon={time_horizon}\"] = 1\n",
    "X.loc[(X[\"Days_hospital_to_outcome\"] > time_horizon), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X.loc[(X[\"is_dead\"] == 0), f\"is_dead_at_time_horizon={time_horizon}\"] = 0\n",
    "X[f\"is_dead_at_time_horizon={time_horizon}\"] = X[f\"is_dead_at_time_horizon={time_horizon}\"].astype(int)\n",
    "\n",
    "X.drop(columns=[\"is_dead\", \"Days_hospital_to_outcome\"], inplace=True) # drop survival columns as they are not needed for a classification problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a region mapper, which maps the region encoding to the real values. These can be found in `synthetic-data-lab/data/Brazil_COVID/Brazil_COVID_data.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mappings from region index to region\n",
    "region_mapper = {\n",
    "    0: \"Central-West\",\n",
    "    1: \"North\",\n",
    "    2: \"Northeast\",\n",
    "    3: \"South\",\n",
    "    4: \"Southeast\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are acting as officials from Central-West Brazil, split the data into data from our region and data from other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_region_index = 0\n",
    "\n",
    "# Flatten region to simulate the scenario where we don't know where the data has come from, we just have our data and other data\n",
    "other_region_index = 1 if our_region_index != 1 else 0\n",
    "X.loc[X[\"Region\"] != our_region_index, \"Region\"] = other_region_index\n",
    "print(X[\"Region\"].value_counts().rename({our_region_index: \"Our region\", 1:\"Another region\"}))\n",
    "\n",
    "X_our_region_only = X.loc[X[\"Region\"] == our_region_index].copy()\n",
    "X_other_regions = X.loc[X[\"Region\"] != our_region_index].copy()\n",
    "\n",
    "display(X_our_region_only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The problem\n",
    "\n",
    "Lets see how a model trained just on our data from the Central-West region performs.\n",
    "\n",
    "### 5.1 Set up the data splits using train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load classifier\n",
    "Load the trained xgboost classifier, which has been trained on Central-West data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_{region_mapper[our_region_index]}_data.json\"\n",
    "# xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate classifier\n",
    "Now print the performance of the model trained on only Central-West data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we are significantly over-fitting due to the very small dataset. The performance does not look as good as it could be.\n",
    "\n",
    "## 6. Concatenation, not Augmentation\n",
    "\n",
    "Lets test our assertion that we can't just use all the training data and apply it to our region.\n",
    "\n",
    "### 6.1 Set up the training and testing data sets for the model\n",
    "\n",
    "Make sure the training sets come from the all region dataset, but the test sets come from our region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "X_train, _, y_train, _ = train_test_split(X_in, y, random_state=4)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "y = X_our_region_only[\"is_dead_at_time_horizon=14\"]\n",
    "X_in = X_our_region_only.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X_in, y, random_state=4)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load the model trained on data from all regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_real_all_data.json\"\n",
    "# xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Show the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_accuracy_score_train = accuracy_score(y_train, xgb_model.predict(X_train))\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Evaluating accuracy: train set: {calculated_accuracy_score_train} | test set: {calculated_accuracy_score_test}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our accuracy does improve, but we can do better! And there may well be cases where there is a greater co-variate shift that impacts this accuracy to a much greater extent. It is also worth bearing in mind that there are contexts where the above approach is not even an option, such as in the case of only partially overlapping (or missing) features.\n",
    "\n",
    "## 7. The Solution\n",
    "\n",
    "Augment this dataset with the use of a RadialGAN.\n",
    "\n",
    "### 7.1 Load the data\n",
    "\n",
    "First, lets load the super-set of data from all regions into the GerericDataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericDataLoader(\n",
    "    X, # X is the dataframe which is a superset of all region data\n",
    "    target_column=\"is_dead_at_time_horizon=14\", # The column containing the labels to predict\n",
    "    sensitive_features=[\"Ethnicity\"], # The sensitive features in the dataset\n",
    "    domain_column=\"Region\", # This labels the domain that each record is from. Where it is `0` it is from our small dataset.\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Load/Create the synthetic data model\n",
    "Lets use a RadialGan to augment the data. We need to load the plugin and then fit it to the dataloader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(\"saved_models\")\n",
    "prefix = \"augmentation\"\n",
    "model=\"radialgan\"\n",
    "n_iter = 100\n",
    "random_state = 42\n",
    "\n",
    "# Define saved model name\n",
    "save_file = outdir / f\"{prefix}.{model}_numericalised_{region_mapper[our_region_index]}_n_iter={n_iter}_rnd={random_state}.bkp\"\n",
    "# Load if available\n",
    "if Path(save_file).exists():\n",
    "    syn_model = serialization.load_from_file(save_file)\n",
    "# create and fit if not available\n",
    "else:\n",
    "    syn_model = Plugins().get(model, n_iter=n_iter, random_state=random_state)\n",
    "    syn_model.fit(loader)\n",
    "    serialization.save_to_file(save_file, syn_model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Augment the dataset\n",
    "\n",
    "Lets use our synthetic model to generate some data and use it to augment our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen_records = 1000\n",
    "\n",
    "synth_data = syn_model.generate(n_gen_records, domains=[our_region_index], random_state=random_state)\n",
    "\n",
    "# Now we can augment our original dataset with our new synthetic data\n",
    "augmented_data = pd.concat([\n",
    "    synth_data.dataframe(),\n",
    "    X_our_region_only,\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Set up the data for the classifier\n",
    "Now we need to test a model trained on the augmented dataset, so we need to set up our data splits again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_y = augmented_data[\"is_dead_at_time_horizon=14\"]\n",
    "augmented_X = augmented_data.drop(columns=[\"is_dead_at_time_horizon=14\"])\n",
    "\n",
    "augmented_y.reset_index(drop=True, inplace=True)\n",
    "augmented_X.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=1, \n",
    "    gamma=1, \n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Load the model trained on the whole dataset\n",
    "saved_model_path = AUG_RES_PATH / f\"augmentation_xgboost_augmented_data.json\"\n",
    "# xgb_model.load_model(saved_model_path)\n",
    "\n",
    "# # The saved model was trained with the following code:\n",
    "xgb_model.fit(augmented_X, augmented_y)\n",
    "xgb_model.save_model(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Evaluate the classifiers performance\n",
    "Show the performance of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(augmented_X)\n",
    "calculated_accuracy_score_test = accuracy_score(augmented_y, y_pred)\n",
    "print(f\"Accuracy on training set (augmented data): {calculated_accuracy_score_test:0.4f}\")\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "calculated_accuracy_score_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on real data test set: {calculated_accuracy_score_test:0.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model over-fitting on the training data is significantly reduced and the accuracy that is much higher than for the small dataset comprised solely of data from the Central-West region. We also see a significant improvement over training the model on the superset of the real data.\n",
    "\n",
    "## 8. Extension\n",
    "Use the code block below as a space to complete the extension exercises below.\n",
    "\n",
    "### Can you generate some more augmented datasets to answer the following questions?\n",
    "1) How much synthetic data should you create for best results?\n",
    "2) How much does changing the RadialGan plugin parameter `n_iter` change the quality of the generated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synth-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b1180d7559eadeaa51f0c23b115f584a6e0cc67e9bc1d662a0e6b39392000a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
